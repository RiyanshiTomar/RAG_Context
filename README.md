# ğŸ¤– RAG Context - AI-Powered Document Chat

A **Retrieval-Augmented Generation (RAG)** chatbot built with **LangChain.js** that enables intelligent conversations with your PDF documents. Supports both **Google Gemini** and **local Ollama models** for flexible deployment.

## âœ¨ Features

- ğŸ“„ **PDF Document Processing** - Load and process PDF files into searchable chunks
- ğŸ” **Vector Search** - Semantic search using Pinecone vector database
- ğŸ’¬ **Conversational Memory** - Maintains context across multiple questions
- ğŸŒ **Dual Model Support**:
  - **Google Gemini** - Cloud-based, powerful AI (gemini-2.0-flash-exp)
  - **Ollama** - Local inference, no API limits (llama3.2, phi3, gemma2)
- âš¡ **Smart Chunking** - Recursive text splitting with overlap for better context
- ğŸ¯ **Context-Aware Responses** - Uses conversation history for relevant answers

## ğŸ—ï¸ Architecture

```
PDF Document â†’ Load â†’ Split into Chunks â†’ Embed â†’ Store in Pinecone
                                                         â†“
User Question â†’ Embed â†’ Search Similar Chunks â†’ Retrieve Context
                                                         â†“
                               Context + History + Question â†’ LLM â†’ Answer
```

## ğŸ“‹ Prerequisites

- Node.js (v18+)
- **Pinecone account** (free tier available)
- **Either**:
  - Google Gemini API key, OR
  - Ollama installed locally

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/RiyanshiTomar/RAG_Context.git
cd RAG_Context
```

### 2. Install Dependencies

```bash
npm install
```

### 3. Set Up Environment Variables

Create a `.env` file in the root directory:

```env
# Pinecone Configuration (Required)
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_INDEX_NAME=your_index_name

# Google Gemini (if using cloud model)
GEMINI_API_KEY=your_gemini_api_key_here

# Configuration Flags
UPLOAD_TO_PINECONE=false  # Set to 'true' for first-time upload
USE_OLLAMA=false          # Set to 'true' to use local Ollama instead of Gemini
```

### 4. First-Time Setup: Upload Documents

Set `UPLOAD_TO_PINECONE=true` in your `.env` file, then run:

```bash
node indexing.js
```

This will:
- Load the PDF from `sample-report.pdf`
- Split it into 1000-character chunks
- Create embeddings
- Upload to Pinecone

### 5. Start Chatting

Set `UPLOAD_TO_PINECONE=false` in `.env`, then run:

```bash
node indexing.js
```

## ğŸ’¡ Usage

### Interactive Commands

- **Ask questions**: Type your question and press Enter
- **`history`** - View conversation history
- **`clear`** - Clear conversation history
- **`exit`** - Quit the application

### Example Session

```
=== RAG Chat Bot Ready ===
Type "exit" to quit, "history" to see conversation history, "clear" to clear history

Ask me anything--> What is this document about?
ğŸ” Processing your question...
  â†’ Creating embedding...
  âœ“ Embedding created
  â†’ Searching Pinecone...
  âœ“ Found 5 relevant documents
  â†’ Asking AI...
  âœ“ Response received

This document is a comprehensive report about...

Ask me anything--> Can you summarize the key findings?
...
```

## âš™ï¸ Configuration

### Model Selection

**Google Gemini (Cloud)**:
```env
USE_OLLAMA=false
GEMINI_API_KEY=your_key
```

**Ollama (Local)**:
```env
USE_OLLAMA=true
# Make sure Ollama is running: ollama serve
```

### Supported Ollama Models

- `llama3.2` - Balanced performance (default)
- `phi3` - Lightweight, fast
- `gemma2:2b` - Efficient, good quality

### Advanced Settings

Edit constants in `indexing.js`:

```javascript
const CHUNK_SIZE = 1000;       // Size of text chunks
const CHUNK_OVERLAP = 200;     // Overlap between chunks
const TOP_K_RESULTS = 5;       // Number of results to retrieve
```

## ğŸ“ Project Structure

```
RAG_Context/
â”œâ”€â”€ indexing.js           # Main application
â”œâ”€â”€ package.json          # Dependencies
â”œâ”€â”€ .env                  # Environment variables (not tracked)
â”œâ”€â”€ .gitignore           # Git ignore rules
â”œâ”€â”€ sample-report.pdf    # Your PDF document
â””â”€â”€ README.md            # This file
```

## ğŸ”§ Technologies Used

- **LangChain.js** - RAG orchestration framework
- **Pinecone** - Vector database for similarity search
- **Google Gemini** - Cloud LLM (gemini-2.0-flash-exp)
- **Ollama** - Local LLM inference
- **pdf-parse** - PDF text extraction
- **readline-sync** - Interactive CLI interface

## ğŸ› Troubleshooting

### "No relevant documents found"
- Ensure you uploaded documents first (`UPLOAD_TO_PINECONE=true`)
- Check Pinecone index name matches your `.env`

### Ollama connection errors
- Verify Ollama is running: `ollama serve`
- Check models are installed: `ollama list`
- Install models if needed: `ollama pull llama3.2`

### API quota exceeded (Gemini)
- Switch to Ollama: `USE_OLLAMA=true`
- Or wait for quota reset

## ğŸ“ License

ISC

## ğŸ‘¤ Author

**Riyanshi Tomar**

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!

## â­ Show Your Support

Give a â­ï¸ if this project helped you!

---

**Built with â¤ï¸ using LangChain and AI**
